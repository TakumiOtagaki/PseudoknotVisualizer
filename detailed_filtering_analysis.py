#!/usr/bin/env python3
"""
Detailed Filtering Analysis Script

å„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ãƒã‚§ãƒ¼ãƒ³æ•°ã®å¤‰åŒ–ã‚’è©³ç´°ã«è¿½è·¡
"""

import json
from pathlib import Path
from collections import Counter

def detailed_filtering_analysis():
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°åˆ†æž"""
    
    # åˆæœŸãƒ‡ãƒ¼ã‚¿
    dataset_dir = Path("analysis/datasets/BGSU__M__All__A__4_0__pdb_3_396")
    pdb_files = list(dataset_dir.glob("*.pdb"))
    initial_chain_count = len(pdb_files)
    
    print("=" * 80)
    print("ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è©³ç´°åˆ†æž")
    print("=" * 80)
    
    # è§£æžçµæžœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦åˆ†æž
    analysis_files = [
        ("analysis/pseudoknot_analysis_dssr_all.json", "DSSR (All)"),
        ("analysis/pseudoknot_analysis_rnaview_all.json", "RNAView (All)")
    ]
    
    for json_file, description in analysis_files:
        if not Path(json_file).exists():
            print(f"âŒ {json_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            continue
        
        with open(json_file, 'r') as f:
            results = json.load(f)
        
        print(f"ðŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ•ãƒ­ãƒ¼åˆ†æž ({description})")
        print("-" * 80)
    
        # Step 1: åˆæœŸãƒ•ã‚¡ã‚¤ãƒ«æ•°
        print(f"Step 1 - åˆæœŸPDBãƒ•ã‚¡ã‚¤ãƒ«æ•°: {initial_chain_count}")
        
        # Step 2: å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        processed_count = len(results)
        removed_files = initial_chain_count - processed_count
        print(f"Step 2 - å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {processed_count} (é™¤å¤–: {removed_files})")
        
        # Step 3: è§£æžæˆåŠŸ (output_existsãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒãªã„å ´åˆã¯ã€total_bp_count >= 0ã§åˆ¤å®š)
        if 'output_exists' in results[0] if results else False:
            successful_analyses = [r for r in results if r.get('output_exists', False)]
        else:
            # RNAViewã®å ´åˆï¼štotal_bp_countãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã‚’æˆåŠŸã¨ã™ã‚‹
            successful_analyses = [r for r in results if 'total_bp_count' in r]
        
        analysis_failures = processed_count - len(successful_analyses)
        print(f"Step 3 - è§£æžæˆåŠŸ: {len(successful_analyses)} (å¤±æ•—: {analysis_failures})")
        
        # Step 4: å¡©åŸºå¯¾ãŒè¦‹ã¤ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«
        chains_with_basepairs = [r for r in results if r.get('total_bp_count', 0) > 0]
        no_basepairs = len(successful_analyses) - len(chains_with_basepairs)
        print(f"Step 4 - å¡©åŸºå¯¾ç™ºè¦‹: {len(chains_with_basepairs)} (å¡©åŸºå¯¾ãªã—: {no_basepairs})")
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è©³ç´°çµ±è¨ˆ
        analyze_filtering_details(results, description)
        print()

def analyze_filtering_details(results, description):
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è©³ç´°çµ±è¨ˆã®åˆ†æž"""
    print("\nðŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è©³ç´°çµ±è¨ˆ")
    print("-" * 80)    # ç•°å¸¸ãƒšã‚¢ã®ç¨®é¡žåˆ¥çµ±è¨ˆ
    self_pairs_count = 0
    duplicate_pairs_count = 0
    other_abnormal_count = 0
    
    for result in results:
        abnormal_pairs = result.get('abnormal_pairs', [])
        dup_canonical_pairs = result.get('dup_canonical_pairs', [])
        
        # è‡ªå·±ãƒšã‚¢ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for pair in abnormal_pairs:
            if isinstance(pair, list) and len(pair) == 2 and pair[0] == pair[1]:
                self_pairs_count += 1
            else:
                other_abnormal_count += 1
        
        duplicate_pairs_count += len(dup_canonical_pairs)
    
    print(f"è‡ªå·±ãƒšã‚¢ (i,i) æ¤œå‡ºæ•°: {self_pairs_count}")
    print(f"é‡è¤‡canonicalå¡©åŸºå¯¾æ¤œå‡ºæ•°: {duplicate_pairs_count}")
    print(f"ãã®ä»–ç•°å¸¸ãƒšã‚¢æ¤œå‡ºæ•°: {other_abnormal_count}")

def analyze_distribution_stats(results, description):
    """åˆ†å¸ƒçµ±è¨ˆã®åˆ†æž"""
    chains_with_basepairs = [r for r in results if r.get('total_bp_count', 0) > 0]
    
    if not chains_with_basepairs:
        print("å¡©åŸºå¯¾ãŒè¦‹ã¤ã‹ã£ãŸãƒã‚§ãƒ¼ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # å¡©åŸºå¯¾æ•°åˆ†å¸ƒ
    print("\nðŸ“ˆ å¡©åŸºå¯¾æ•°åˆ†å¸ƒ")
    print("-" * 80)
    
    bp_counts = [r['total_bp_count'] for r in chains_with_basepairs]
    canonical_bp_counts = [r.get('total_canonical_bp_count', 0) for r in chains_with_basepairs]
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ çš„è¡¨ç¤º
    bp_ranges = [(0, 10), (11, 20), (21, 50), (51, 100), (101, 200), (201, 500), (500, float('inf'))]
    
    for min_bp, max_bp in bp_ranges:
        if max_bp == float('inf'):
            count = len([bp for bp in bp_counts if bp > min_bp])
            range_str = f"{min_bp}+"
        else:
            count = len([bp for bp in bp_counts if min_bp <= bp <= max_bp])
            range_str = f"{min_bp}-{max_bp}"
        
        percentage = count / len(bp_counts) * 100 if bp_counts else 0
        print(f"å¡©åŸºå¯¾æ•° {range_str:>8}: {count:>4} ãƒã‚§ãƒ¼ãƒ³ ({percentage:>5.1f}%)")
    
    # Canonical ratioåˆ†æž
    print("\nðŸŽ¯ Canonicalå¡©åŸºå¯¾æ¯”çŽ‡åˆ†æž")
    print("-" * 80)
    
    canonical_ratios = []
    for result in chains_with_basepairs:
        total_bp = result['total_bp_count']
        canonical_bp = result.get('total_canonical_bp_count', 0)
        if total_bp > 0:
            ratio = canonical_bp / total_bp
            canonical_ratios.append(ratio)
    
    if canonical_ratios:
        avg_canonical_ratio = sum(canonical_ratios) / len(canonical_ratios)
        high_canonical = len([r for r in canonical_ratios if r >= 0.8])
        medium_canonical = len([r for r in canonical_ratios if 0.5 <= r < 0.8])
        low_canonical = len([r for r in canonical_ratios if r < 0.5])
        
        print(f"å¹³å‡canonicalæ¯”çŽ‡: {avg_canonical_ratio:.3f}")
        print(f"é«˜canonicalæ¯”çŽ‡ (â‰¥80%): {high_canonical} ãƒã‚§ãƒ¼ãƒ³ ({high_canonical/len(canonical_ratios)*100:.1f}%)")
        print(f"ä¸­canonicalæ¯”çŽ‡ (50-79%): {medium_canonical} ãƒã‚§ãƒ¼ãƒ³ ({medium_canonical/len(canonical_ratios)*100:.1f}%)")
        print(f"ä½Žcanonicalæ¯”çŽ‡ (<50%): {low_canonical} ãƒã‚§ãƒ¼ãƒ³ ({low_canonical/len(canonical_ratios)*100:.1f}%)")
    
    # æœ€çµ‚ã‚µãƒžãƒªãƒ¼
    print("\nðŸ“‹ æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚µãƒžãƒªãƒ¼")
    print("-" * 80)
    retention_rates = [
        ("åˆæœŸãƒ•ã‚¡ã‚¤ãƒ«æ•°", initial_chain_count, 100.0),
        ("å‡¦ç†å¯¾è±¡", processed_count, processed_count/initial_chain_count*100),
        ("è§£æžæˆåŠŸ", len(successful_analyses), len(successful_analyses)/initial_chain_count*100),
        ("å¡©åŸºå¯¾ç™ºè¦‹", len(chains_with_basepairs), len(chains_with_basepairs)/initial_chain_count*100),
    ]
    
    for stage, count, percentage in retention_rates:
        print(f"{stage:>12}: {count:>4} ãƒã‚§ãƒ¼ãƒ³ ({percentage:>5.1f}%)")

if __name__ == "__main__":
    detailed_filtering_analysis()
