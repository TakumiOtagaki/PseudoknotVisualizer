#!/usr/bin/env python3
"""
Multi Base Pairing Entry Extractor

DSSRè§£æçµæœã‹ã‚‰"(i, j) ã¨ (i, j') ã¨ã„ã£ãŸ multi base pairing ã‚’æŒã£ãŸã‚¨ãƒ³ãƒˆãƒª"ã‚’æŠ½å‡ºã—ã€
æ–°ã—ã„JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Step by step ã§æ…é‡ã«å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import json
from pathlib import Path

def extract_multi_pairing_entries(input_json_file, output_json_file):
    """
    multi base pairingã‚’æŒã¤ã‚¨ãƒ³ãƒˆãƒªã‚’æŠ½å‡º
    
    Args:
        input_json_file (str): å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        output_json_file (str): å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    
    Returns:
        int: æŠ½å‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒˆãƒªæ•°
    """
    print(f"ğŸ“– å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {input_json_file}")
    
    with open(input_json_file, 'r') as f:
        all_results = json.load(f)
    
    print(f"ğŸ“Š ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(all_results)}")
    
    # multi base pairingã‚’æŒã¤ã‚¨ãƒ³ãƒˆãƒªã‚’æŠ½å‡º
    multi_pairing_entries = []
    
    for entry in all_results:
        dup_canonical_pairs = entry.get('dup_canonical_pairs', [])
        
        # dup_canonical_pairsãŒç©ºã§ãªã„ã‚¨ãƒ³ãƒˆãƒªã‚’æŠ½å‡º
        if dup_canonical_pairs:
            multi_pairing_entries.append(entry)
            print(f"âœ… Multi pairingç™ºè¦‹: {entry['pdb_id']} - {len(dup_canonical_pairs)} pairs")
    
    print(f"\nğŸ¯ Multi base pairing ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(multi_pairing_entries)}")
    
    # çµæœã‚’ä¿å­˜
    with open(output_json_file, 'w') as f:
        json.dump(multi_pairing_entries, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {output_json_file}")
    
    return len(multi_pairing_entries)

def analyze_multi_pairing_stats(entries):
    """
    multi base pairingã‚¨ãƒ³ãƒˆãƒªã®çµ±è¨ˆã‚’åˆ†æ
    """
    print("\nğŸ“ˆ Multi Base Pairing çµ±è¨ˆåˆ†æ")
    print("=" * 50)
    
    total_dup_pairs = 0
    layer_counts = []
    bp_counts = []
    canonical_ratios = []
    
    for entry in entries:
        dup_pairs = len(entry.get('dup_canonical_pairs', []))
        total_dup_pairs += dup_pairs
        
        layer_count = entry.get('pseudoknot_layer_count', 0)
        layer_counts.append(layer_count)
        
        total_bp = entry.get('total_bp_count', 0)
        canonical_bp = entry.get('total_canonical_bp_count', 0)
        bp_counts.append(total_bp)
        
        if total_bp > 0:
            canonical_ratios.append(canonical_bp / total_bp)
    
    print(f"ç·é‡è¤‡canonicalå¡©åŸºå¯¾æ•°: {total_dup_pairs}")
    print(f"å¹³å‡é‡è¤‡æ•°/ã‚¨ãƒ³ãƒˆãƒª: {total_dup_pairs / len(entries):.1f}")
    print(f"å¹³å‡pseudoknot layeræ•°: {sum(layer_counts) / len(layer_counts):.1f}")
    print(f"å¹³å‡å¡©åŸºå¯¾æ•°: {sum(bp_counts) / len(bp_counts):.1f}")
    print(f"å¹³å‡canonicalæ¯”ç‡: {sum(canonical_ratios) / len(canonical_ratios):.3f}")
    
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°åˆ†å¸ƒ
    from collections import Counter
    layer_dist = Counter(layer_counts)
    print(f"\nPseudoknot Layeråˆ†å¸ƒ:")
    for layers, count in sorted(layer_dist.items()):
        print(f"  {layers} layers: {count} ã‚¨ãƒ³ãƒˆãƒª")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” Multi Base Pairing ã‚¨ãƒ³ãƒˆãƒªæŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # å…¥åŠ›ãƒ»å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
    input_file = "analysis/pseudoknot_analysis_dssr_all.json"
    output_file = "analysis/multi_pairing_entries_dssr.json"
    
    # Step 1: ã‚¨ãƒ³ãƒˆãƒªæŠ½å‡º
    extracted_count = extract_multi_pairing_entries(input_file, output_file)
    
    # Step 2: æŠ½å‡ºã—ãŸã‚¨ãƒ³ãƒˆãƒªã‚’å†èª­ã¿è¾¼ã¿ã—ã¦çµ±è¨ˆåˆ†æ
    if extracted_count > 0:
        with open(output_file, 'r') as f:
            multi_pairing_entries = json.load(f)
        
        analyze_multi_pairing_stats(multi_pairing_entries)
    else:
        print("âš ï¸ Multi base pairingã‚¨ãƒ³ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    main()
