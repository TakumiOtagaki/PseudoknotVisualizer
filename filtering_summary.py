#!/usr/bin/env python3
"""
Analysis Summary Script

BGSU__M__All__A__4_0__pdb_3_396ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è§£æçµæœã‚’çµ±è¨ˆçš„ã«ã¾ã¨ã‚ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰å¾Œã®ãƒã‚§ãƒ¼ãƒ³æ•°ã®å¤‰åŒ–ã‚’è¿½è·¡ã—ã¾ã™ã€‚
"""

import json
from pathlib import Path

def load_analysis_results(json_file):
    """è§£æçµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    with open(json_file, 'r') as f:
        return json.load(f)

def analyze_filtering_steps():
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°åˆ†æ"""
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®PDBãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ã®ç·ãƒã‚§ãƒ¼ãƒ³æ•°ï¼‰
    dataset_dir = Path("analysis/datasets/BGSU__M__All__A__4_0__pdb_3_396")
    pdb_files = list(dataset_dir.glob("*.pdb"))
    initial_chain_count = len(pdb_files)
    
    print("=" * 60)
    print("BGSU__M__All__A__4_0__pdb_3_396 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æ")
    print("=" * 60)
    print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ã®ç·ãƒã‚§ãƒ¼ãƒ³æ•°: {initial_chain_count}")
    print()
    
    # è§£æçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    analysis_files = [
        ("analysis/pseudoknot_analysis_dssr_all.json", "DSSR (All)"),
        ("analysis/pseudoknot_analysis_dssr_canonical_only.json", "DSSR (Canonical Only)"),
        ("analysis/pseudoknot_analysis_rnaview_all.json", "RNAView (All)"),
        ("analysis/pseudoknot_analysis_rnaview_canonical_only.json", "RNAView (Canonical Only)")
    ]
    
    for json_file, description in analysis_files:
        if not Path(json_file).exists():
            print(f"âš ï¸  {description}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ ({json_file})")
            continue
            
        results = load_analysis_results(json_file)
        
        print(f"ğŸ“Š {description}")
        print("-" * 50)
        
        # åŸºæœ¬çµ±è¨ˆ
        total_processed = len(results)
        successful_chains = [r for r in results if r.get('output_exists', False)]
        chains_with_basepairs = [r for r in results if r.get('total_bp_count', 0) > 0]
        
        print(f"å‡¦ç†ã•ã‚ŒãŸãƒã‚§ãƒ¼ãƒ³æ•°: {total_processed}")
        print(f"è§£ææˆåŠŸãƒã‚§ãƒ¼ãƒ³æ•°: {len(successful_chains)}")
        print(f"å¡©åŸºå¯¾ãŒè¦‹ã¤ã‹ã£ãŸãƒã‚§ãƒ¼ãƒ³æ•°: {len(chains_with_basepairs)}")
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è©³ç´°
        chains_with_abnormal = [r for r in results if r.get('abnormal_pairs', [])]
        chains_with_duplicates = [r for r in results if r.get('dup_canonical_pairs', [])]
        
        print(f"ç•°å¸¸ãªå¡©åŸºå¯¾ãŒæ¤œå‡ºã•ã‚ŒãŸãƒã‚§ãƒ¼ãƒ³æ•°: {len(chains_with_abnormal)}")
        print(f"é‡è¤‡canonicalå¡©åŸºå¯¾ãŒæ¤œå‡ºã•ã‚ŒãŸãƒã‚§ãƒ¼ãƒ³æ•°: {len(chains_with_duplicates)}")
        
        # å¡©åŸºå¯¾çµ±è¨ˆ
        if chains_with_basepairs:
            total_bp_counts = [r['total_bp_count'] for r in chains_with_basepairs]
            canonical_bp_counts = [r.get('total_canonical_bp_count', 0) for r in chains_with_basepairs]
            
            print(f"å¹³å‡å¡©åŸºå¯¾æ•°: {sum(total_bp_counts) / len(total_bp_counts):.1f}")
            print(f"å¹³å‡canonicalå¡©åŸºå¯¾æ•°: {sum(canonical_bp_counts) / len(canonical_bp_counts):.1f}")
            print(f"æœ€å¤§å¡©åŸºå¯¾æ•°: {max(total_bp_counts)}")
            print(f"æœ€å°å¡©åŸºå¯¾æ•°: {min(total_bp_counts)}")
        
        print()
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®è©³ç´°åˆ†æ
    print("ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è©³ç´°åˆ†æ")
    print("=" * 60)
    
    # DSSR Allã®çµæœã‚’ä½¿ã£ã¦è©³ç´°åˆ†æ
    dssr_all_file = "analysis/pseudoknot_analysis_dssr_all.json"
    if Path(dssr_all_file).exists():
        results = load_analysis_results(dssr_all_file)
        
        print("DSSRã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°:")
        print("1. è‡ªå·±ãƒšã‚¢ (i=i) ã®é™¤å¤–")
        print("2. é‡è¤‡ã—ãŸä½ç½®ã®å¡©åŸºå¯¾ã®å‡¦ç†:")
        print("   - (i,j) ã¨ (i,j') ã®ã‚ˆã†ãªé‡è¤‡ãŒã‚ã£ãŸå ´åˆ:")
        print("     â€¢ ä¸€æ–¹ãŒcanonicalã€ä»–æ–¹ãŒnon-canonicalãªã‚‰ã€canonicalã‚’ä¿æŒ")
        print("     â€¢ ä¸¡æ–¹ã¨ã‚‚non-canonicalãªã‚‰ã€ä¸¡æ–¹ã¨ã‚‚é™¤å¤–")
        print("     â€¢ ä¸¡æ–¹ã¨ã‚‚canonicalãªã‚‰ã€é‡è¤‡canonicalãƒšã‚¢ã¨ã—ã¦è¨˜éŒ²")
        print("3. æ–¹å‘ã®çµ±ä¸€ (i,j) ã¨ (j,i) ã®é‡è¤‡å‡¦ç†")
        print()
        
        # ç•°å¸¸ãƒšã‚¢ã®è©³ç´°çµ±è¨ˆ
        total_abnormal_pairs = 0
        total_duplicate_canonical = 0
        
        for result in results:
            abnormal_count = len(result.get('abnormal_pairs', []))
            duplicate_count = len(result.get('dup_canonical_pairs', []))
            total_abnormal_pairs += abnormal_count
            total_duplicate_canonical += duplicate_count
        
        print(f"ç·ç•°å¸¸ãƒšã‚¢æ•°: {total_abnormal_pairs}")
        print(f"ç·é‡è¤‡canonicalãƒšã‚¢æ•°: {total_duplicate_canonical}")
        
        # ãƒã‚§ãƒ¼ãƒ³æ•°ã®å¤‰åŒ–ã‚’è¿½è·¡
        print("\nğŸ“ˆ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒã‚§ãƒ¼ãƒ³æ•°å¤‰åŒ–:")
        print(f"åˆæœŸãƒã‚§ãƒ¼ãƒ³æ•°: {initial_chain_count}")
        print(f"å‡¦ç†ã•ã‚ŒãŸãƒã‚§ãƒ¼ãƒ³æ•°: {len(results)}")
        print(f"è§£ææˆåŠŸãƒã‚§ãƒ¼ãƒ³æ•°: {len([r for r in results if r.get('output_exists', False)])}")
        print(f"æœ‰åŠ¹ãªå¡©åŸºå¯¾ãŒã‚ã‚‹ãƒã‚§ãƒ¼ãƒ³æ•°: {len([r for r in results if r.get('total_bp_count', 0) > 0])}")
        
        retention_rate = len([r for r in results if r.get('total_bp_count', 0) > 0]) / initial_chain_count * 100
        print(f"æœ€çµ‚ä¿æŒç‡: {retention_rate:.1f}%")

if __name__ == "__main__":
    analyze_filtering_steps()
